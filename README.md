
# Digital AI Strategy - Week **7**

## Course Overview

Welcome to the **Digital AI Strategy** course! This repository contains materials and resources for **Week 7**, focusing on **Digital and AI Strategy Implementation**.

### What's Included

- **DAIS_Week7_RAG.py**: Interactive Streamlit application with RAG (Retrieval Augmented Generation) capabilities
- **Course Materials**: Vector database and supporting resources

## Getting Started

1. Install dependencies: `pip install -r requirements.txt`
2. Run the Streamlit app: `streamlit run DAIS_Week5_RAG.py`
3. Enter your API key for your preferred LLM provider
4. Start asking questions about Platform Strategy concepts!

## ğŸ—„ï¸ Vector Database Implementation

### Introduction to Vector Databases

Vector databases are specialized storage systems designed to handle high-dimensional vector embeddings efficiently. In the context of this Digital AI Strategy course, vector databases serve several critical purposes:

- **Semantic Search**: Enable finding conceptually similar content rather than just keyword matches
- **Knowledge Retrieval**: Power the RAG (Retrieval Augmented Generation) system to provide contextually relevant answers
- **Course Material Organization**: Store and index course content for intelligent querying and exploration
- **Scalable Information Access**: Handle large volumes of educational content efficiently

The implementation uses **ChromaDB**, a popular open-source vector database, combined with **HuggingFace embeddings** to create a sophisticated retrieval system for course materials.

### How Vector Embeddings Work

Vector embeddings transform text content into numerical representations (vectors) that capture semantic meaning. Similar concepts end up close together in vector space, enabling:

1. **Semantic similarity matching** - Find related concepts even with different wording
2. **Context-aware retrieval** - Understand the meaning behind queries
3. **Efficient similarity search** - Quickly find relevant course materials

### Current Implementation Overview

The application uses the following vector database setup:

- **Embedding Model**: `all-MiniLM-L6-v2` (HuggingFace Sentence Transformers)
- **Vector Store**: ChromaDB with local persistence
- **Storage Location**: `./Week_5_1Oct2025/` directory
- **Retrieval Method**: Similarity search with top-k results (k=5)

### Step-by-Step Guide: Generating Vector Embeddings

#### 1. Prepare Your Course Content

```python
# Example: Prepare documents for embedding
from langchain.schema import Document

# Your course materials as text chunks
course_materials = [
Â  Â  "The three main product-to-platform transformation methods are Launchpad, Accelerator, and Blueprint...",
Â  Â  "Network effects lead to exponential growth by increasing value with each new user...",
Â  Â  "Platform governance is key to mitigating risks from third-party developers and sellers...",
Â  Â  # Add more content
]

# Convert to Document objects
documents = [
Â  Â  Document(page_content=content, metadata={"source": f"chunk_{i}"})
Â  Â  for i, content in enumerate(course_materials)
]
````

#### 2\. Initialize the Embedding Model

```python
from langchain_huggingface import HuggingFaceEmbeddings

# Initialize the same embedding model used in the application
embedding_model = HuggingFaceEmbeddings(
Â  Â  model_name="all-MiniLM-L6-v2",
Â  Â  model_kwargs={'device': 'cpu'},Â  # Use 'cuda' if GPU available
Â  Â  encode_kwargs={'normalize_embeddings': True}
)
```

#### 3\. Create and Populate the Vector Database

```python
from langchain_community.vectorstores import Chroma

# Create new vector database
vectorstore = Chroma.from_documents(
Â  Â  documents=documents,
Â  Â  embedding=embedding_model,
Â  Â  persist_directory="./your_course_vectordb"
)

# Persist the database to disk
vectorstore.persist()
```

#### 4\. Test the Vector Database

```python
# Test similarity search
query = "What are the risks of inviting third-party sellers?"
results = vectorstore.similarity_search(query, k=3)

for i, doc in enumerate(results):
Â  Â  print(f"Result {i+1}: {doc.page_content[:100]}...")
```

### Setting Up and Configuring the Vector Database

#### Option 1: Using the Pre-built Database

The repository includes a pre-built vector database in the `Week_5_27Oct2025` directory. To use it:

1.  Ensure the directory exists and contains the ChromaDB files
2.  Update the `PERSIST_DIRECTORY` variable in `DAIS_Week5_RAG.py`:
    Â  Â `python Â  Â PERSIST_DIRECTORY = "./Week_5_27Oct2025" Â  Â `
3.  Run the application - it will automatically load the existing database

#### Option 2: Creating Your Own Database

1.  **Prepare your content**: Organize course materials into text chunks
2.  **Install required packages**: Ensure you have all dependencies from `requirements.txt`
3.  **Create the database**:

<!-- end list -->

```python
import sys
import pysqlite3
sys.modules["sqlite3"] = pysqlite3Â  # Required for ChromaDB compatibility

from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# Your implementation here
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Load your documents
documents = [...]Â  # Your course content as Document objects

# Create vectorstore
vectorstore = Chroma.from_documents(
Â  Â  documents=documents,
Â  Â  embedding=embedding_model,
Â  Â  persist_directory="./your_custom_vectordb"
)
```

4.  **Update the application**: Modify `PERSIST_DIRECTORY` to point to your new database

### Configuration Options and Customization

#### Embedding Model Options

You can customize the embedding model based on your needs:

```python
# Option 1: Multilingual support
embedding_model = HuggingFaceEmbeddings(
Â  Â  model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Option 2: Higher accuracy (larger model)
embedding_model = HuggingFaceEmbeddings(
Â  Â  model_name="sentence-transformers/all-mpnet-base-v2"
)

# Option 3: Domain-specific models
embedding_model = HuggingFaceEmbeddings(
Â  Â  model_name="sentence-transformers/allenai-specter"Â  # Scientific papers
)
```

#### Search Configuration

Customize the retrieval behavior:

```python
# In the run_similarity_search function
def run_similarity_search(query):
Â  Â  # Adjust k for more/fewer results
Â  Â  results = vectorstore.similarity_search(
Â  Â  Â  Â  query,Â 
Â  Â  Â  Â  k=10,Â  # Return top 10 instead of 5
Â  Â  Â  Â  filter={"source": "specific_topic"}Â  # Optional filtering
Â  Â  )
Â  Â  return results

# Alternative: Use similarity search with score threshold
def run_similarity_search_with_score(query):
Â  Â  results = vectorstore.similarity_search_with_score(
Â  Â  Â  Â  query,Â 
Â  Â  Â  Â  k=5,
Â  Â  Â  Â  score_threshold=0.7Â  # Only return results above similarity threshold
Â  Â  )
Â  Â  return [doc for doc, score in results if score > 0.7]
```

#### Database Configuration

Customize ChromaDB settings:

```python
# Advanced ChromaDB configuration
vectorstore = Chroma(
Â  Â  persist_directory="./custom_vectordb",
Â  Â  embedding_function=embedding_model,
Â  Â  collection_name="course_materials",Â  # Custom collection name
Â  Â  collection_metadata={"description": "Digital AI Strategy Course Content"}
)
```

### Examples of Extending Functionality

#### 1\. Adding New Course Materials

```python
# Add new documents to existing database
new_documents = [
Â  Â  Document(page_content="New course content...", metadata={"week": "5"})
]

vectorstore.add_documents(new_documents)
vectorstore.persist()
```

#### 2\. Multi-Collection Setup

```python
# Create separate collections for different course weeks
week1_vectorstore = Chroma(
Â  Â  persist_directory="./vectordb",
Â  Â  embedding_function=embedding_model,
Â  Â  collection_name="week1_materials"
)

week2_vectorstore = Chroma(
Â  Â  persist_directory="./vectordb",
Â  Â  embedding_function=embedding_model,
Â  Â  collection_name="week2_materials"
)
```

#### 3\. Metadata Filtering

```python
# Search with metadata filters
def search_by_topic(query, topic):
Â  Â  results = vectorstore.similarity_search(
Â  Â  Â  Â  query,
Â  Â  Â  Â  k=5,
Â  Â  Â  Â  filter={"topic": topic}
Â  Â  )
Â  Â  return results

# Usage
platform_results = search_by_topic("network effects", "Platform Strategy")
governance_results = search_by_topic("rules for third-parties", "Ecosystem Governance")
```

#### 4\. Hybrid Search (Vector + Keyword)

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# Combine vector similarity with keyword search
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
keyword_retriever = BM25Retriever.from_documents(documents)

ensemble_retriever = EnsembleRetriever(
Â  Â  retrievers=[vector_retriever, keyword_retriever],
Â  Â  weights=[0.7, 0.3]Â  # 70% vector, 30% keyword
)
```

#### 5\. Custom Similarity Metrics

```python
# Use different distance metrics
vectorstore = Chroma(
Â  Â  persist_directory="./vectordb",
Â  Â  embedding_function=embedding_model,
Â  Â  collection_metadata={"hnsw:space": "cosine"}Â  # Options: l2, ip, cosine
)
```

### Performance Optimization Tips

1.  **Batch Processing**: Process documents in batches for better performance
2.  **GPU Acceleration**: Use CUDA for embedding generation if available
3.  **Chunking Strategy**: Optimize text chunk size (typically 200-1000 tokens)
4.  **Caching**: Cache embeddings to avoid recomputation
5.  **Index Optimization**: Tune ChromaDB parameters for your use case

### Troubleshooting Common Issues

#### Issue 1: SQLite3 Compatibility

```python
# Fix for ChromaDB SQLite issues
import sys
import pysqlite3
sys.modules["sqlite3"] = pysqlite3
```

#### Issue 2: CUDA/GPU Issues

```python
# Force CPU usage if GPU issues
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
```

#### Issue 3: Memory Issues with Large Datasets

```python
# Process in smaller batches
batch_size = 100
for i in range(0, len(documents), batch_size):
Â  Â  batch = documents[i:i+batch_size]
Â  Â  vectorstore.add_documents(batch)
Â  Â  vectorstore.persist()
```

### ğŸ”§ Helper Script: create\_embeddings.py

The repository includes a powerful helper script that simplifies creating vector embeddings from your course content. This script allows you to easily convert .md or .txt files into vector embeddings and store them in a ChromaDB database.

#### Features

  - **Multiple File Support**: Process multiple .md or .txt files at once
  - **Customizable Chunking**: Adjust chunk sizes and overlap for optimal performance
  - **GPU/CPU Support**: Automatically detect and use GPU acceleration when available
  - **Database Management**: Create new databases or update existing ones
  - **Error Handling**: Comprehensive error handling with user-friendly feedback
  - **Testing**: Built-in database testing with sample queries

#### Basic Usage

```bash
# Create embeddings from a single file
python create_embeddings.py course_material.md

# Process multiple files
python create_embeddings.py file1.md file2.txt file3.md

# Process all markdown files in a directory
python create_embeddings.py *.md
```

#### Advanced Usage

```bash
# Use custom chunk size and enable GPU
python create_embeddings.py --chunk-size 1000 --use-gpu files/*.md

# Specify custom database location
python create_embeddings.py --db-path ./my_vectordb files/*.txt

# Use a different embedding model
python create_embeddings.py --model sentence-transformers/all-mpnet-base-v2 *.md

# Create with custom collection name
python create_embeddings.py --collection-name "week7_materials" week7/*.md
```

#### Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--model` | HuggingFace model name for embeddings | `all-MiniLM-L6-v2` |
| `--chunk-size` | Size of text chunks for processing | `500` |
| `--chunk-overlap` | Overlap between text chunks | `50` |
| `--use-gpu` | Use GPU for embedding generation if available | `false` |
| `--db-path` | Directory to store the vector database | `./course_vectordb` |
| `--collection-name` | Name of the vector database collection | `course_materials` |
| `--test-query` | Query to test the database with | `"platform governance"` |
| `--no-test` | Skip testing the vector database after creation | `false` |

#### Integration with DAIS\_Week5\_RAG.py

After creating your vector database with the helper script, update the `PERSIST_DIRECTORY` variable in `DAIS_Week5_RAG.py`:

```python
# Update this line in DAIS_Week5_RAG.py
PERSIST_DIRECTORY = "./your_custom_vectordb"Â  # Path from --db-path option
```

#### Example Workflow

1.  **Prepare your content**: Organize course materials into .md or .txt files
2.  **Create embeddings**: Use the helper script to process your files
3.  **Update the app**: Point the Streamlit app to your new database
4.  **Test**: Run the app and verify that your content is searchable

<!-- end list -->

```bash
# Step 1: Create embeddings from your course materials
python create_embeddings.py --chunk-size 800 --use-gpu course_materials/*.md

# Step 2: Update DAIS_Week5_RAG.py to use your database
# PERSIST_DIRECTORY = "./course_vectordb"

# Step 3: Run the Streamlit app
streamlit run DAIS_Week7_RAG.py
```

#### Troubleshooting the Helper Script

**Issue 1: Model Download Errors**

```bash
# Ensure you have internet connectivity for first-time model download
# Models are cached locally after first download
```

**Issue 2: Memory Issues with Large Files**

```bash
# Use smaller chunk sizes for large files
python create_embeddings.py --chunk-size 300 large_file.md
```

**Issue 3: GPU Issues**

```bash
# Force CPU usage if GPU issues occur
python create_embeddings.py large_file.mdÂ  # GPU detection is automatic
```

### Best Practices for Course Material Implementation

1.  **Content Preparation**:
    Â  Â - Clean and preprocess text content
    Â  Â - Maintain consistent formatting
    Â  Â - Include relevant metadata (topics, weeks, difficulty level)

2.  **Chunking Strategy**:
    Â  Â - Split content into logical, coherent chunks
    Â  Â - Overlap chunks slightly to maintain context
    Â  Â - Size chunks appropriately (aim for 200-500 tokens)

3.  **Database Management**:
    Â  Â - Regular backups of vector databases
    Â  Â - Version control for database schemas
    Â  Â - Monitor database size and performance

4.  **Quality Assurance**:
    Â  Â - Test retrieval quality with sample queries
    Â  Â - Validate that similar concepts are found together
    Â  Â - Monitor and improve relevance over time

-----

## Sample Questions to Explore Platform Strategy Concepts

This collection of sample questions is designed to help you chat with this bot to understand Digital Transformation, Business Strategy, and Organizational Design through relatable and creative analogies, focusing on the real-world application of frameworks like the 10-Step Digital Strategy Implementation Blueprint and Jay R. Galbraith's Star Modelâ„¢ .

ğŸ¦¸â€â™‚ï¸ Pop Culture & Marvel Universe
Marvel-Inspired Questions (Transformation & Structure Focus)
"If an organization undergoing digital transformation is like the Avengers, the Star Modelâ„¢ is their team charter. Explain how the five points of the Star Modelâ„¢ (Strategy, Structure, Processes, Rewards, People) are represented by different roles or elements within the Marvel Cinematic Universe (e.g., Nick Fury's strategy, the S.H.I.E.L.D. structure, etc.)."

"The 10-Step Digital Strategy Implementation Framework is like Doctor Strange's attempt to execute a complex plan across multiple dimensions. Which three steps of the framework would represent the greatest 'magic' (requiring deep technical expertise) and which three represent the greatest 'political challenge' (requiring organizational alignment)?"

ğŸ‘¶ Child-Friendly Explanations
Simple Language Questions (Case Study & Concepts Focus)
"Vodafone's goal is to 'Automate and improve customer care.' Explain the core trade-off (the 'good' vs. the 'bad') of using automated digital tools to a five-year-old, using the analogy of a robot friend who is very fast but can't give hugs."

"How would you explain the difference between ABB's broad goal of creating 'continuous value' versus CNH Industrial's focused goal of 'predictive maintenance services,' using a simple analogy involving a doctor (ABB) who gives overall wellness advice versus a mechanic (CNH) who fixes one specific engine part?"

ğŸ® Gaming & Interactive Analogies
Gaming-Inspired Questions (Implementation Focus)
"Compare the challenges of aligning the five elements of the Star Modelâ„¢ during transformation to the challenge of managing resources and skill trees in a complex strategy game (like Civilization or Crusader Kings). Specifically, what happens if you upgrade your 'Strategy' tech without upgrading your 'People' skill tree?"

"In a business simulation game, what are the three essential KPIs (Key Performance Indicators) you must track to measure the success of a digital transformation, and how do they differ if your goal is primarily to increase efficiency versus increase customer value?"

ğŸ§± Building & Construction Analogies
LEGO & Construction Questions (Framework Focus)
"Compare the 10-Step Digital Strategy Implementation Framework to building a skyscraper. Which step is the architectural blueprint, which step is laying the foundation (IT infrastructure), and which step is the final inspection (Measuring & Adjusting)? Describe the consequence of skipping any one of these steps."

ğŸ™ï¸ Ecosystem & Community Analogies
City Building Questions (Organizational Focus)
"When a company redesigns its organization using the Star Modelâ„¢, itâ€™s like redesigning a city. Compare the 'Processes' element of the Star Modelâ„¢ to the city's traffic light system and the 'Structure' element to the city's government departments. Why is it disastrous to change one without changing the other?"

ğŸµ Creative & Artistic Analogies
Orchestra & Music Questions (Integration Focus)
"How do Digital Transformation, Business Strategy, and Organizational Design work together in a successful company, like instruments in an orchestra? Specifically, if the Business Strategy is the conductor, what role does the Star Modelâ„¢ play as the musical score (sheet music) in ensuring every section (department) plays together?"

ğŸ” Advanced Integration Questions
Cross-Concept Connections
"The challenge for CNH Industrial is to embed digital services into its physical equipment. How would the company use the concepts of the 10-Step Implementation Framework, the Star Modelâ„¢'s 'Processes' element, and the 'predictive maintenance' goal to redesign its entire supply chain and field service operations?"

ğŸ¯ Application-Focused Questions
Real-World Application
"Based on the case study, ABB's goal is to create 'continuous value.' If you were the CIO, how would you specifically apply the 'Metrics and Measures' step of the 10-Step Framework to quantify 'continuous value' for a long-standing industrial customer, and how would this metric drive organizational rewards?"

How to Use These Questions
Choose your favorite analogies: Start with contexts you're most familiar with.

Layer the complexity: Begin with simple comparisons and add details.

Mix and match: Combine different analogies for richer understanding.

Create your own: Use these as inspiration for personal analogies.

Test with the RAG system: Ask these questions in the course application.

Discuss with peers: Share analogies and learn from others' perspectives.

Tips for Creating Your Own Analogies
Start with what you know: Use your hobbies, interests, and experiences.

Focus on relationships: How do components interact and depend on each other?

Consider scale: How do systems grow and evolve over time?

Think about problems: What challenges exist and how are they solved?

Explore benefits: What value is created and for whom?

Remember: The goal is to make complex concepts accessible and memorable. The best analogies are the ones that resonate with your personal experience and help you see familiar patterns in new contexts!
